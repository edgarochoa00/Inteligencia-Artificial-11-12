{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC1WNlAq56UZ"
      },
      "outputs": [],
      "source": [
        "# Programa de Reconocimiento de Emociones Faciales\n",
        "# Procesamiento y Data Augmentation\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar albumentations\n",
        "try:\n",
        "    import albumentations as A\n",
        "    ALBUMENTATIONS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ALBUMENTATIONS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    COLAB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    COLAB_AVAILABLE = False\n",
        "\n",
        "\n",
        "# CONFIGURACI√ìN INICIAL\n",
        "\n",
        "\n",
        "# Configuraci√≥n de GPU\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
        "\n",
        "# Configuraci√≥n de semillas\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Par√°metros\n",
        "IMG_SIZE = 224\n",
        "EMOTIONS = ['angry', 'neutral', 'fear', 'happy', 'sad']\n",
        "\n",
        "def upload_and_extract_zip():\n",
        "    \"\"\"Subir y extraer archivo ZIP\"\"\"\n",
        "    if COLAB_AVAILABLE:\n",
        "        uploaded = files.upload()\n",
        "        zip_filename = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        zip_filename = input(\"Ruta del archivo ZIP: \")\n",
        "        if not os.path.exists(zip_filename):\n",
        "            raise FileNotFoundError(f\"Archivo no encontrado: {zip_filename}\")\n",
        "\n",
        "    extract_path = './dataset'\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    return extract_path\n",
        "\n",
        "def load_csv_data(dataset_path):\n",
        "    \"\"\"Cargar datos desde archivos CSV\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                csv_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "                except:\n",
        "                    try:\n",
        "                        df = pd.read_csv(csv_path, encoding='latin-1')\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                df['base_path'] = root\n",
        "                all_data.append(df)\n",
        "\n",
        "    if not all_data:\n",
        "        raise ValueError(\"No se encontraron archivos CSV v√°lidos\")\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "def find_emotion_column(df):\n",
        "    \"\"\"Encontrar columna de emociones\"\"\"\n",
        "    possible_cols = ['emotion', 'label', 'class', 'category', 'target']\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col.lower() in possible_cols:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def create_verified_dataset(df, emotion_column):\n",
        "    \"\"\"Crear dataset verificado con correspondencia imagen-etiqueta correcta\"\"\"\n",
        "    print(\"üîç Creando dataset verificado...\")\n",
        "\n",
        "    verified_data = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Encontrar columna de filename\n",
        "        filename_col = None\n",
        "        for col in ['filename', 'image', 'file', 'image_name']:\n",
        "            if col in row and pd.notna(row[col]):\n",
        "                filename_col = col\n",
        "                break\n",
        "\n",
        "        if not filename_col:\n",
        "            continue\n",
        "\n",
        "        emotion = row[emotion_column]\n",
        "        if emotion not in EMOTIONS:\n",
        "            continue\n",
        "\n",
        "        # Construir ruta de imagen\n",
        "        image_path = os.path.join(row['base_path'], str(row[filename_col]))\n",
        "\n",
        "        # Verificar que existe\n",
        "        if os.path.exists(image_path):\n",
        "            verified_data.append({\n",
        "                'image_path': image_path,\n",
        "                'emotion': emotion,\n",
        "                'original_index': idx\n",
        "            })\n",
        "\n",
        "    print(f\"‚úÖ Dataset verificado: {len(verified_data)} im√°genes v√°lidas de {len(df)} total\")\n",
        "    return verified_data\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
        "    \"\"\"Cargar y preprocesar imagen con mejor manejo de errores\"\"\"\n",
        "    try:\n",
        "        # Intentar cargar con cv2\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            # Intentar con PIL como respaldo\n",
        "            pil_image = Image.open(image_path)\n",
        "            if pil_image.mode != 'RGB':\n",
        "                pil_image = pil_image.convert('RGB')\n",
        "            image = np.array(pil_image)\n",
        "        else:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Redimensionar\n",
        "        processed_image = cv2.resize(image, target_size)\n",
        "        processed_image = processed_image.astype(np.float32) / 255.0\n",
        "\n",
        "        return processed_image, image\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error cargando {image_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def show_sample_images(verified_data, num_samples=12):\n",
        "    \"\"\"Mostrar im√°genes de muestra del dataset verificado\"\"\"\n",
        "    if len(verified_data) < num_samples:\n",
        "        num_samples = len(verified_data)\n",
        "\n",
        "    # Seleccionar muestras aleatorias\n",
        "    sample_indices = np.random.choice(len(verified_data), num_samples, replace=False)\n",
        "\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        data_point = verified_data[idx]\n",
        "        processed_img, original_img = load_and_preprocess_image(data_point['image_path'])\n",
        "\n",
        "        if original_img is not None:\n",
        "            axes[i].imshow(original_img)\n",
        "            axes[i].set_title(f\"Emoci√≥n: {data_point['emotion']}\", fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "        else:\n",
        "            axes[i].text(0.5, 0.5, 'Error al cargar', ha='center', va='center')\n",
        "            axes[i].set_title(f\"Error: {data_point['emotion']}\", fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Muestra de Im√°genes del Dataset Verificado', fontsize=16, y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "def create_augmentation_pipeline():\n",
        "    \"\"\"Pipeline de augmentaci√≥n con Albumentations\"\"\"\n",
        "    if ALBUMENTATIONS_AVAILABLE:\n",
        "        return A.Compose([\n",
        "            A.Rotate(limit=20, p=0.7),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "            A.GaussianBlur(blur_limit=3, p=0.3),\n",
        "            A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.3),\n",
        "        ])\n",
        "    return None\n",
        "\n",
        "def create_keras_augmentation():\n",
        "    \"\"\"Pipeline de augmentaci√≥n con Keras\"\"\"\n",
        "    return ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        brightness_range=[0.8, 1.2]\n",
        "    )\n",
        "\n",
        "def apply_augmentation_albumentations(image, transform):\n",
        "    \"\"\"Aplicar augmentaci√≥n con Albumentations\"\"\"\n",
        "    try:\n",
        "        if image.dtype == np.float32:\n",
        "            image_uint8 = (image * 255).astype(np.uint8)\n",
        "        else:\n",
        "            image_uint8 = image\n",
        "\n",
        "        augmented = transform(image=image_uint8)\n",
        "        return augmented['image'].astype(np.float32) / 255.0\n",
        "    except:\n",
        "        return image\n",
        "\n",
        "def load_dataset_from_verified_data(verified_data, batch_size=100):\n",
        "    \"\"\"\n",
        "    FUNCI√ìN CLAVE: Cargar dataset manteniendo correspondencia perfecta\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Cargando {len(verified_data)} im√°genes verificadas...\")\n",
        "\n",
        "    images = []\n",
        "    emotions = []\n",
        "    failed_count = 0\n",
        "\n",
        "    # Procesar en lotes para mostrar progreso\n",
        "    for i in range(0, len(verified_data), batch_size):\n",
        "        batch_end = min(i + batch_size, len(verified_data))\n",
        "        batch_data = verified_data[i:batch_end]\n",
        "\n",
        "        print(f\"   Procesando lote {i//batch_size + 1}/{(len(verified_data)-1)//batch_size + 1}\")\n",
        "\n",
        "        for data_point in batch_data:\n",
        "            processed_img, _ = load_and_preprocess_image(data_point['image_path'], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "            if processed_img is not None:\n",
        "                images.append(processed_img)\n",
        "                emotions.append(data_point['emotion'])\n",
        "            else:\n",
        "                failed_count += 1\n",
        "\n",
        "    print(f\"‚úÖ Cargadas exitosamente: {len(images)}\")\n",
        "    print(f\"‚ùå Fallos: {failed_count}\")\n",
        "\n",
        "    return np.array(images), np.array(emotions)\n",
        "\n",
        "def augment_dataset(X_train, y_train, augment_factor=2, method='keras'):\n",
        "    \"\"\"Aplicar data augmentation manteniendo correspondencia\"\"\"\n",
        "    print(f\"üìà Aplicando augmentaci√≥n (factor {augment_factor}) con {method}...\")\n",
        "\n",
        "    original_count = len(X_train)\n",
        "    augmented_needed = (original_count * (augment_factor - 1))  # Corregido el c√°lculo\n",
        "\n",
        "    if augmented_needed <= 0:\n",
        "        return X_train, y_train\n",
        "\n",
        "    X_augmented = []\n",
        "    y_augmented = []\n",
        "\n",
        "    if method == 'albumentations' and ALBUMENTATIONS_AVAILABLE:\n",
        "        transform = create_augmentation_pipeline()\n",
        "\n",
        "        for i in range(augmented_needed):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"   Generadas {i}/{augmented_needed} im√°genes aumentadas\")\n",
        "\n",
        "            # Seleccionar imagen aleatoria\n",
        "            idx = np.random.randint(0, len(X_train))\n",
        "            original_image = X_train[idx]\n",
        "            original_label = y_train[idx]\n",
        "\n",
        "            # Aplicar augmentaci√≥n\n",
        "            augmented_image = apply_augmentation_albumentations(original_image, transform)\n",
        "\n",
        "            X_augmented.append(augmented_image)\n",
        "            y_augmented.append(original_label)\n",
        "\n",
        "    elif method == 'keras':\n",
        "        datagen = create_keras_augmentation()\n",
        "        generated = 0\n",
        "\n",
        "        while generated < augmented_needed:\n",
        "            if generated % 100 == 0:\n",
        "                print(f\"   Generadas {generated}/{augmented_needed} im√°genes aumentadas\")\n",
        "\n",
        "            # Seleccionar imagen aleatoria\n",
        "            idx = np.random.randint(0, len(X_train))\n",
        "            original_image = X_train[idx]\n",
        "            original_label = y_train[idx]\n",
        "\n",
        "            # Aplicar augmentaci√≥n\n",
        "            img_batch = np.expand_dims(original_image, axis=0)\n",
        "            aug_iter = datagen.flow(img_batch, batch_size=1)\n",
        "            augmented_img = next(aug_iter)[0]\n",
        "\n",
        "            X_augmented.append(augmented_img)\n",
        "            y_augmented.append(original_label)\n",
        "            generated += 1\n",
        "\n",
        "    # Combinar datos originales y aumentados\n",
        "    X_final = np.concatenate([X_train, np.array(X_augmented)], axis=0)\n",
        "    y_final = np.concatenate([y_train, np.array(y_augmented)], axis=0)\n",
        "\n",
        "    print(f\"‚úÖ Augmentaci√≥n completada: {len(X_final)} im√°genes totales\")\n",
        "    return X_final, y_final\n",
        "\n",
        "def verify_dataset_integrity(X, y, label_encoder, dataset_name, num_samples=5):\n",
        "    \"\"\"Verificar integridad del dataset\"\"\"\n",
        "    print(f\"\\nüîç Verificando integridad de {dataset_name}:\")\n",
        "    print(f\"   Total de im√°genes: {len(X)}\")\n",
        "    print(f\"   Total de etiquetas: {len(y)}\")\n",
        "\n",
        "    # Verificar distribuci√≥n\n",
        "    if len(y) > 0:\n",
        "        unique_labels, counts = np.unique(y, return_counts=True)\n",
        "        print(\"   Distribuci√≥n:\")\n",
        "        for label_idx, count in zip(unique_labels, counts):\n",
        "            emotion_name = label_encoder.classes_[label_idx]\n",
        "            print(f\"     {emotion_name}: {count} im√°genes\")\n",
        "\n",
        "        # Mostrar algunas muestras aleatorias para verificaci√≥n visual\n",
        "        if len(X) >= num_samples:\n",
        "            sample_indices = np.random.choice(len(X), num_samples, replace=False)\n",
        "\n",
        "            fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "            if num_samples == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, idx in enumerate(sample_indices):\n",
        "                axes[i].imshow(X[idx])\n",
        "                emotion_name = label_encoder.classes_[y[idx]]\n",
        "                axes[i].set_title(f'{emotion_name}\\n(c√≥digo: {y[idx]})', fontsize=10)\n",
        "                axes[i].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'Verificaci√≥n de {dataset_name}', fontsize=12, y=1.05)\n",
        "            plt.show()\n",
        "\n",
        "def process_dataset_corrected(dataset_path, augment_factor=3, show_samples=True):\n",
        "    \"\"\"\n",
        "    FUNCI√ìN PRINCIPAL CORREGIDA - Procesar dataset manteniendo correspondencia perfecta\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando procesamiento corregido del dataset...\")\n",
        "\n",
        "    # 1. Cargar datos CSV\n",
        "    print(\"\\nüìÇ Paso 1: Cargando datos CSV...\")\n",
        "    df = load_csv_data(dataset_path)\n",
        "    emotion_column = find_emotion_column(df)\n",
        "\n",
        "    if emotion_column is None:\n",
        "        raise ValueError(\"No se encontr√≥ columna de emociones\")\n",
        "\n",
        "    print(f\"‚úÖ Columna de emociones encontrada: '{emotion_column}'\")\n",
        "\n",
        "    # 2. Crear dataset verificado\n",
        "    print(\"\\nüîç Paso 2: Verificando correspondencia imagen-etiqueta...\")\n",
        "    verified_data = create_verified_dataset(df[df[emotion_column].isin(EMOTIONS)], emotion_column)\n",
        "\n",
        "    if len(verified_data) == 0:\n",
        "        raise ValueError(\"No se encontraron datos v√°lidos\")\n",
        "\n",
        "    # Mostrar distribuci√≥n inicial\n",
        "    emotions_list = [data['emotion'] for data in verified_data]\n",
        "    unique_emotions, counts = np.unique(emotions_list, return_counts=True)\n",
        "    print(\"\\nüìä Distribuci√≥n inicial:\")\n",
        "    for emotion, count in zip(unique_emotions, counts):\n",
        "        print(f\"   {emotion}: {count} im√°genes\")\n",
        "\n",
        "    # 3. Mostrar muestras si se solicita\n",
        "    if show_samples:\n",
        "        print(\"\\nüñºÔ∏è Paso 3: Mostrando muestras del dataset...\")\n",
        "        show_sample_images(verified_data)\n",
        "\n",
        "    # 4. Dividir datos ANTES de cargar im√°genes (clave para mantener correspondencia)\n",
        "    print(\"\\n‚úÇÔ∏è Paso 4: Dividiendo dataset...\")\n",
        "\n",
        "    # Extraer rutas y emociones para divisi√≥n estratificada\n",
        "    image_paths = [data['image_path'] for data in verified_data]\n",
        "    emotions = [data['emotion'] for data in verified_data]\n",
        "\n",
        "    # Divisi√≥n estratificada\n",
        "    indices = list(range(len(verified_data)))\n",
        "\n",
        "    # Train/temp split (80/20)\n",
        "    train_indices, temp_indices = train_test_split(\n",
        "        indices, test_size=0.2, random_state=SEED,\n",
        "        stratify=emotions\n",
        "    )\n",
        "\n",
        "    # Temp -> Val/Test split (10/10 del total)\n",
        "    temp_emotions = [emotions[i] for i in temp_indices]\n",
        "    val_indices, test_indices = train_test_split(\n",
        "        temp_indices, test_size=0.5, random_state=SEED,\n",
        "        stratify=temp_emotions\n",
        "    )\n",
        "\n",
        "    # Crear subsets verificados\n",
        "    train_data = [verified_data[i] for i in train_indices]\n",
        "    val_data = [verified_data[i] for i in val_indices]\n",
        "    test_data = [verified_data[i] for i in test_indices]\n",
        "\n",
        "    print(f\"   Entrenamiento: {len(train_data)} im√°genes\")\n",
        "    print(f\"   Validaci√≥n: {len(val_data)} im√°genes\")\n",
        "    print(f\"   Prueba: {len(test_data)} im√°genes\")\n",
        "\n",
        "    # 5. Cargar im√°genes manteniendo correspondencia\n",
        "    print(\"\\nüîÑ Paso 5: Cargando im√°genes...\")\n",
        "\n",
        "    X_train, y_train_text = load_dataset_from_verified_data(train_data)\n",
        "    X_val, y_val_text = load_dataset_from_verified_data(val_data)\n",
        "    X_test, y_test_text = load_dataset_from_verified_data(test_data)\n",
        "\n",
        "    # 6. Codificar etiquetas\n",
        "    print(\"\\nüè∑Ô∏è Paso 6: Codificando etiquetas...\")\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(EMOTIONS)  # Ajustar a todas las emociones posibles\n",
        "\n",
        "    y_train = label_encoder.transform(y_train_text)\n",
        "    y_val = label_encoder.transform(y_val_text)\n",
        "    y_test = label_encoder.transform(y_test_text)\n",
        "\n",
        "    print(\"Mapeo de etiquetas:\")\n",
        "    for i, emotion in enumerate(label_encoder.classes_):\n",
        "        print(f\"   {emotion} -> {i}\")\n",
        "\n",
        "    # 7. Verificar integridad antes de augmentaci√≥n\n",
        "    print(\"\\nüîç Paso 7: Verificando integridad antes de augmentaci√≥n...\")\n",
        "    verify_dataset_integrity(X_train, y_train, label_encoder, \"Entrenamiento Original\")\n",
        "    verify_dataset_integrity(X_val, y_val, label_encoder, \"Validaci√≥n\")\n",
        "    verify_dataset_integrity(X_test, y_test, label_encoder, \"Prueba\")\n",
        "\n",
        "    # 8. Aplicar data augmentation\n",
        "    print(f\"\\n Paso 8: Aplicando data augmentation (factor {augment_factor})...\")\n",
        "\n",
        "    if augment_factor > 1:\n",
        "        method = 'albumentations' if ALBUMENTATIONS_AVAILABLE else 'keras'\n",
        "        print(f\"M√©todo de augmentaci√≥n: {method}\")\n",
        "\n",
        "        X_train_aug, y_train_aug = augment_dataset(\n",
        "            X_train, y_train,\n",
        "            augment_factor=augment_factor,\n",
        "            method=method\n",
        "        )\n",
        "    else:\n",
        "        X_train_aug, y_train_aug = X_train, y_train\n",
        "\n",
        "    # 9. Verificaci√≥n final\n",
        "    print(\"\\n‚úÖ Paso 9: Verificaci√≥n final...\")\n",
        "    verify_dataset_integrity(X_train_aug, y_train_aug, label_encoder, \"Entrenamiento Final\")\n",
        "\n",
        "    print(\"\\n Procesamiento completado exitosamente!\")\n",
        "    print(f\" Resumen final:\")\n",
        "    print(f\"   Entrenamiento: {len(X_train_aug)} im√°genes\")\n",
        "    print(f\"   Validaci√≥n: {len(X_val)} im√°genes\")\n",
        "    print(f\"   Prueba: {len(X_test)} im√°genes\")\n",
        "    print(f\"   Clases: {len(label_encoder.classes_)} ({', '.join(label_encoder.classes_)})\")\n",
        "\n",
        "    return {\n",
        "        'train_data': (X_train_aug, y_train_aug),\n",
        "        'val_data': (X_val, y_val),\n",
        "        'test_data': (X_test, y_test),\n",
        "        'label_encoder': label_encoder,\n",
        "        'num_classes': len(label_encoder.classes_),\n",
        "        'class_names': label_encoder.classes_\n",
        "    }\n",
        "\n",
        "def detailed_verification(result, num_samples=10):\n",
        "    \"\"\"Verificaci√≥n detallada con visualizaci√≥n\"\"\"\n",
        "    if result is None:\n",
        "        print(\"‚ùå No hay datos para verificar\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüî¨ VERIFICACI√ìN DETALLADA DE {num_samples} MUESTRAS\")\n",
        "\n",
        "    X_train, y_train = result['train_data']\n",
        "    label_encoder = result['label_encoder']\n",
        "\n",
        "    # Seleccionar muestras de cada clase si es posible\n",
        "    samples_per_class = max(1, num_samples // len(label_encoder.classes_))\n",
        "    selected_indices = []\n",
        "\n",
        "    for class_idx in range(len(label_encoder.classes_)):\n",
        "        class_indices = np.where(y_train == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            sample_count = min(samples_per_class, len(class_indices))\n",
        "            selected = np.random.choice(class_indices, sample_count, replace=False)\n",
        "            selected_indices.extend(selected)\n",
        "\n",
        "    # Limitar al n√∫mero total solicitado\n",
        "    if len(selected_indices) > num_samples:\n",
        "        selected_indices = np.random.choice(selected_indices, num_samples, replace=False)\n",
        "\n",
        "    # Mostrar verificaci√≥n\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    print(\"\\nDetalles de las muestras seleccionadas:\")\n",
        "    for i, idx in enumerate(selected_indices[:10]):\n",
        "        image = X_train[idx]\n",
        "        label_code = y_train[idx]\n",
        "        emotion_name = label_encoder.classes_[label_code]\n",
        "\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].set_title(f'√çndice: {idx}\\nC√≥digo: {label_code}\\nEmoci√≥n: {emotion_name}',\n",
        "                         fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "        print(f\"   {i+1:2d}. √çndice {idx:5d} | C√≥digo {label_code} | Emoci√≥n '{emotion_name}'\")\n",
        "\n",
        "    # Ocultar ejes no utilizados\n",
        "    for i in range(len(selected_indices), 10):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('üî¨ Verificaci√≥n Detallada: Correspondencia Imagen-Etiqueta', fontsize=16, y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # Mostrar estad√≠sticas de distribuci√≥n\n",
        "    print(f\"\\nüìä ESTAD√çSTICAS FINALES:\")\n",
        "    unique_labels, counts = np.unique(y_train, return_counts=True)\n",
        "    total_images = len(y_train)\n",
        "\n",
        "    for label_idx, count in zip(unique_labels, counts):\n",
        "        emotion_name = label_encoder.classes_[label_idx]\n",
        "        percentage = (count / total_images) * 100\n",
        "        print(f\"   {emotion_name:8s}: {count:5d} im√°genes ({percentage:5.1f}%)\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal corregida\"\"\"\n",
        "    try:\n",
        "        # Cargar dataset\n",
        "        print(\"üìÇ Cargando dataset...\")\n",
        "        dataset_path = upload_and_extract_zip()\n",
        "\n",
        "        # Procesar datos con funci√≥n corregida\n",
        "        result = process_dataset_corrected(\n",
        "            dataset_path,\n",
        "            augment_factor=3,\n",
        "            show_samples=True\n",
        "        )\n",
        "\n",
        "        # Verificaci√≥n detallada\n",
        "        detailed_verification(result, num_samples=10)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# ========================================\n",
        "# EJECUTAR\n",
        "# ========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = main()\n",
        "\n",
        "    if result:\n",
        "        print(\"\\nüöÄ ¬°DATOS LISTOS PARA ENTRENAMIENTO!\")\n",
        "        print(\"üí° Accede a los datos con:\")\n",
        "        print(\"   - result['train_data'] para entrenamiento\")\n",
        "        print(\"   - result['val_data'] para validaci√≥n\")\n",
        "        print(\"   - result['test_data'] para prueba\")\n",
        "        print(\"   - result['label_encoder'] para decodificar etiquetas\")\n",
        "    else:\n",
        "        print(\"‚ùå El procesamiento fall√≥. Revisa los errores anteriores.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}